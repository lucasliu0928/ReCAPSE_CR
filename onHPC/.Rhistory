#3.Prediction
#Method 1: predict use AI model
#Method 2: predict as negative/pos/non_obv(use AI)
################################################################################
#Prediction
pred_df_neg <- prediction_2method_func(test_neg_df,features,mod_optimal,"NEG")
features
tops
top_fs
source('~/Desktop/DrChen_Projects/ReCAPSE_Project/ReCAPSE_Code/onHPC/16D_Prediction_TestData.R')
print(check)
source('~/Desktop/DrChen_Projects/ReCAPSE_Project/ReCAPSE_Code/onHPC/16B_Train_Xgboost_Topfeatures.R')
################################################################################
#3.Prediction
#Method 1: predict use AI model
#Method 2: predict as negative/pos/non_obv(use AI)
################################################################################
#Prediction
pred_df_neg <- prediction_2method_func(test_neg_df,top_fs,mod_optimal,"NEG")
pred_df_pos <- prediction_2method_func(test_pos_df,top_fs,mod_optimal,"POS")
pred_df_nonobv <- prediction_2method_func(test_nonobv_df,top_fs,mod_optimal,"nonOBV")
pred_df_all <- rbind(pred_df_neg,pred_df_pos,pred_df_nonobv)
check <- compare_obvs_samples_2methods_perf(pred_df_all,"nonOBV")
print(check)
################################################################################
#3.Prediction
#Method 1: predict use AI model
#Method 2: predict as negative/pos/non_obv(use AI)
################################################################################
#Prediction
features <- top_fs
pred_df_neg <- prediction_2method_func(test_neg_df,features,mod_optimal,"NEG")
pred_df_pos <- prediction_2method_func(test_pos_df,features,mod_optimal,"POS")
pred_df_nonobv <- prediction_2method_func(test_nonobv_df,features,mod_optimal,"nonOBV")
pred_df_all <- rbind(pred_df_neg,pred_df_pos,pred_df_nonobv)
check <- compare_obvs_samples_2methods_perf(pred_df_all,"nonOBV")
print(check)
source('~/Desktop/DrChen_Projects/ReCAPSE_Project/ReCAPSE_Code/onHPC/16B_Train_Xgboost_Topfeatures.R')
################################################################################
#3.Prediction
#Method 1: predict use AI model
#Method 2: predict as negative/pos/non_obv(use AI)
################################################################################
#Prediction
features <- top_fs
pred_df_neg <- prediction_2method_func(test_neg_df,features,mod_optimal,"NEG")
pred_df_pos <- prediction_2method_func(test_pos_df,features,mod_optimal,"POS")
pred_df_nonobv <- prediction_2method_func(test_nonobv_df,features,mod_optimal,"nonOBV")
pred_df_all <- rbind(pred_df_neg,pred_df_pos,pred_df_nonobv)
check <- compare_obvs_samples_2methods_perf(pred_df_all,"nonOBV")
print(check)
source('~/Desktop/DrChen_Projects/ReCAPSE_Project/ReCAPSE_Code/onHPC/16B_Train_Xgboost_Topfeatures.R')
source('~/Desktop/DrChen_Projects/ReCAPSE_Project/ReCAPSE_Code/onHPC/16B_Train_Xgboost_Topfeatures.R')
source('~/Desktop/DrChen_Projects/ReCAPSE_Project/ReCAPSE_Code/onHPC/16B_Train_Xgboost_Topfeatures.R')
source('~/Desktop/DrChen_Projects/ReCAPSE_Project/ReCAPSE_Code/onHPC/16B_Train_Xgboost_Topfeatures.R')
source('~/Desktop/DrChen_Projects/ReCAPSE_Project/ReCAPSE_Code/onHPC/16B_Train_Xgboost_Topfeatures.R')
source('~/Desktop/DrChen_Projects/ReCAPSE_Project/ReCAPSE_Code/onHPC/16B_Train_Xgboost_Topfeatures.R')
source('~/Desktop/DrChen_Projects/ReCAPSE_Project/ReCAPSE_Code/onHPC/16B_Train_Xgboost_Topfeatures.R')
source('~/Desktop/DrChen_Projects/ReCAPSE_Project/ReCAPSE_Code/onHPC/16B_Train_Xgboost_Topfeatures.R')
source('~/Desktop/DrChen_Projects/ReCAPSE_Project/ReCAPSE_Code/onHPC/16B_Train_Xgboost_Topfeatures.R')
source('~/Desktop/DrChen_Projects/ReCAPSE_Project/ReCAPSE_Code/onHPC/16B_Train_Xgboost_Topfeatures.R')
source('~/Desktop/DrChen_Projects/ReCAPSE_Project/ReCAPSE_Code/onHPC/16B_Train_Xgboost_Topfeatures.R')
source("Recapse_Ultility.R")
#this script make predictions for test data using 2 methods:
#1. Method1: AI methods: AI models to predict all test
#2. Method2: hybrid methods: AI model to predict non-obv, obv_neg/pos are predicted as neg/pos
#3. For the results of method1 and method2, then do curve fitting to fine-tune the results
################################################################################
#Set up parallel computing envir
################################################################################
numCores <- detectCores() # get the number of cores available
print(numCores)
registerDoParallel(numCores)  # use multicore, set to the number of our cores
################################################################################
#Data dir
################################################################################
#onHPC
proj_dir  <- "/recapse/intermediate_data/"
#local
#proj_dir  <- "/Users/lucasliu/Desktop/DrChen_Projects/ReCAPSE_Project/ReCAPSE_Intermediate_Data/0610_21/"
#data dir
data_dir1        <- paste0(proj_dir, "15_XGB_Input/")
data_dir2       <- paste0(proj_dir, "16B_Trained_ImportantFeatureModel/")
newout <- "16C_Predictions/Test/"
outdir   <- paste0(proj_dir, newout)
dir.create(file.path(proj_dir, newout), recursive = TRUE)
ds_index<-1
#Create out dir for each ds index
ds_out <- paste0(proj_dir, newout,"train_DS",ds_index,"/prediction_tables/")
dir.create(file.path(ds_out))
dir.create(file.path(ds_out),recursive = TRUE)
ds_out
#Create out dir for each ds index
ds_out <- paste0(proj_dir, newout,"train_DS",ds_index,"/")
dir.create(file.path(ds_out))
outdir
outdir
#Create out dir for each ds index
ds_out <- paste0("DS",ds_index,"/")
dir.create(file.path(outdir, ds_out), recursive = TRUE)
#local
proj_dir  <- "/Users/lucasliu/Desktop/DrChen_Projects/ReCAPSE_Project/ReCAPSE_Intermediate_Data/0610_21/"
#data dir
data_dir1        <- paste0(proj_dir, "15_XGB_Input/")
data_dir2       <- paste0(proj_dir, "16B_Trained_ImportantFeatureModel/")
newout <- "16C_Predictions/Test/"
outdir   <- paste0(proj_dir, newout)
dir.create(file.path(proj_dir, newout), recursive = TRUE)
#Create out dir for each ds index
ds_out <- paste0("DS",ds_index,"/")
dir.create(file.path(outdir, ds_out), recursive = TRUE)
ds_out
paste0(outdir, ds_out, "pred_tb_all.csv")
#Create out dir for each ds index
ds_out <- paste0("DS",ds_index,"/Prediction_Table/")
dir.create(file.path(outdir, ds_out), recursive = TRUE)
paste0(outdir, ds_out, "pred_tb_all.csv")
add_predicted_class_byThreshold <- function(prediction_df,thres_list){
predicted_class_df <- as.data.frame(matrix(NA, nrow = nrow(prediction_df),ncol = length(thres_list)))
colnames(predicted_class_df) <- gsub("\\.","", paste0("PredictedClass_Thres_",thres_list))
for (j in 1:length(thres_list)){
curr_thres <- thres_list[j]
curr_col_name <- gsub("\\.","",paste0("PredictedClass_Thres_",curr_thres))
predicted_class_df[,curr_col_name] <- convert_prediction_function(prediction_df[,"pred"],curr_thres)
}
#Add predicted classes to prediction df
comb_df <- cbind(prediction_df,predicted_class_df)
return(comb_df)
}
add_predicted_class_byThreshold <- function(prediction_df,thres_list){
predicted_class_df <- as.data.frame(matrix(NA, nrow = nrow(prediction_df),ncol = length(thres_list)))
colnames(predicted_class_df) <- gsub("\\.","", paste0("PredictedClass_Thres_",thres_list))
for (j in 1:length(thres_list)){
curr_thres <- thres_list[j]
curr_col_name <- gsub("\\.","",paste0("PredictedClass_Thres_",curr_thres))
predicted_class_df[,curr_col_name] <- convert_prediction_function(prediction_df[,"pred"],curr_thres)
}
#Add predicted classes to prediction df
comb_df <- cbind(prediction_df,predicted_class_df)
return(comb_df)
}
pred_df <- read.csv("/Users/lucasliu/Desktop/DrChen_Projects/ReCAPSE_Project/ReCAPSE_Intermediate_Data/0610_21/16C_Predictions/Test/DS5/Prediction_Table/pred_tb_all.csv",stringsAsFactors = F)
pred_df <- read.csv("/Users/lucasliu/Desktop/DrChen_Projects/ReCAPSE_Project/ReCAPSE_Intermediate_Data/0610_21/16C_Predictions/Test/DS5/Prediction_Table/pred_tb_all.csv",stringsAsFactors = F)
View(pred_df)
colnames(pred_df)
compute_binaryclass_perf_func2
#onHPC
proj_dir  <- "/recapse/intermediate_data/"
#local
proj_dir  <- "/Users/lucasliu/Desktop/DrChen_Projects/ReCAPSE_Project/ReCAPSE_Intermediate_Data/0610_21/"
#data dir
data_dir1        <- paste0(proj_dir, "16_Performance_WithSurgPrimSite_V1_1217updated/Use_ImportantFs_Performance/")
data_dir2        <- paste0(proj_dir, "8_Characteristics2/Patient_Level/")
################################################################################
#3. Load patient level char to get SBCE or not
################################################################################
pts_level_char_df <- read.xlsx(paste0(data_dir2,"/8_PatientLevel_char_WithPossibleMonthsHasNoCodes.xlsx"),sheet = 1)
pred_df <- read.csv("/Users/lucasliu/Desktop/DrChen_Projects/ReCAPSE_Project/ReCAPSE_Intermediate_Data/0610_21/16C_Predictions/Test/DS5/Prediction_Table/pred_tb_all.csv",stringsAsFactors = F)
perf_tb_alltest <- get_perf_table_func(pred_df,pts_level_char_df)
compare_obvs_samples_2methods_perf(pred_df,"nonOBV")
source("Recapse_Ultility.R")
compare_obvs_samples_2methods_perf(pred_df,"nonOBV")
pred_df <- read.csv("/Users/lucasliu/Desktop/DrChen_Projects/ReCAPSE_Project/ReCAPSE_Intermediate_Data/0610_21/16C_Predictions/Test/DS1/Prediction_Table/pred_tb_all.csv",stringsAsFactors = F)
compare_obvs_samples_2methods_perf(pred_df,"nonOBV")
pred_df <- read.csv("/Users/lucasliu/Desktop/DrChen_Projects/ReCAPSE_Project/ReCAPSE_Intermediate_Data/0610_21/16C_Predictions/Test/DS2/Prediction_Table/pred_tb_all.csv",stringsAsFactors = F)
compare_obvs_samples_2methods_perf(pred_df,"nonOBV")
pred_df <- read.csv("/Users/lucasliu/Desktop/DrChen_Projects/ReCAPSE_Project/ReCAPSE_Intermediate_Data/0610_21/16C_Predictions/Test/DS3/Prediction_Table/pred_tb_all.csv",stringsAsFactors = F)
compare_obvs_samples_2methods_perf(pred_df,"nonOBV")
pred_df <- read.csv("/Users/lucasliu/Desktop/DrChen_Projects/ReCAPSE_Project/ReCAPSE_Intermediate_Data/0610_21/16C_Predictions/Test/DS4/Prediction_Table/pred_tb_all.csv",stringsAsFactors = F)
compare_obvs_samples_2methods_perf(pred_df,"nonOBV")
proj_dir <- '/Users/lucasliu/Desktop/DrChen_Projects/ReCAPSE_Project/ReCAPSE_Intermediate_Data/0610_21/16C_Predictions/Test/'
pred_df <- read.csv(paste0(proj_dir, "DS4/Prediction_Table/pred_tb_all.csv"),stringsAsFactors = F)
compare_obvs_samples_2methods_perf(pred_df,"nonOBV")
pred_df <- read.csv(paste0(proj_dir, "DS1/Prediction_Table/pred_tb_all.csv"),stringsAsFactors = F)
proj_dir <- '/Users/lucasliu/Desktop/DrChen_Projects/ReCAPSE_Project/ReCAPSE_Intermediate_Data/0610_21/16C_Predictions/Test/'
pred_df <- read.csv(paste0(proj_dir, "DS1/Prediction_Table/pred_tb_all.csv"),stringsAsFactors = F)
proj_dir <- '/Users/lucasliu/Desktop/DrChen_Projects/ReCAPSE_Project/ReCAPSE_Intermediate_Data/0610_21/16C_Predictions/Test/'
pred_df <- read.csv(paste0(proj_dir, "DS1/Prediction_Table/pred_tb_all.csv"),stringsAsFactors = F)
compare_obvs_samples_2methods_perf(pred_df,"nonOBV")
pred_df <- read.csv(paste0(proj_dir, "DS2/Prediction_Table/pred_tb_all.csv"),stringsAsFactors = F)
compare_obvs_samples_2methods_perf(pred_df,"nonOBV")
pred_df <- read.csv(paste0(proj_dir, "DS3/Prediction_Table/pred_tb_all.csv"),stringsAsFactors = F)
compare_obvs_samples_2methods_perf(pred_df,"nonOBV")
pred_df <- read.csv(paste0(proj_dir, "DS4/Prediction_Table/pred_tb_all.csv"),stringsAsFactors = F)
compare_obvs_samples_2methods_perf(pred_df,"nonOBV")
pred_df <- read.csv(paste0(proj_dir, "DS5/Prediction_Table/pred_tb_all.csv"),stringsAsFactors = F)
compare_obvs_samples_2methods_perf(pred_df,"nonOBV")
pred_df <- read.csv(paste0(proj_dir, "DS6/Prediction_Table/pred_tb_all.csv"),stringsAsFactors = F)
compare_obvs_samples_2methods_perf(pred_df,"nonOBV")
pred_df <- read.csv(paste0(proj_dir, "DS7/Prediction_Table/pred_tb_all.csv"),stringsAsFactors = F)
compare_obvs_samples_2methods_perf(pred_df,"nonOBV")
pred_df <- read.csv(paste0(proj_dir, "DS8/Prediction_Table/pred_tb_all.csv"),stringsAsFactors = F)
compare_obvs_samples_2methods_perf(pred_df,"nonOBV")
pred_df <- read.csv(paste0(proj_dir, "DS9/Prediction_Table/pred_tb_all.csv"),stringsAsFactors = F)
compare_obvs_samples_2methods_perf(pred_df,"nonOBV")
pred_df <- read.csv(paste0(proj_dir, "DS10/Prediction_Table/pred_tb_all.csv"),stringsAsFactors = F)
compare_obvs_samples_2methods_perf(pred_df,"nonOBV")
pred_df <- read.csv(paste0(proj_dir, "DS5/Prediction_Table/pred_tb_all.csv"),stringsAsFactors = F)
compare_obvs_samples_2methods_perf(pred_df,"nonOBV")
colnames(pred_df)
add_predicted_class_byThreshold <- function(prediction_df,thres_list){
predicted_class_df <- as.data.frame(matrix(NA, nrow = nrow(prediction_df),ncol = length(thres_list)))
colnames(predicted_class_df) <- gsub("\\.","", paste0("PredictedClass_Thres_",thres_list))
for (j in 1:length(thres_list)){
curr_thres <- thres_list[j]
curr_col_name <- gsub("\\.","",paste0("PredictedClass_Thres_",curr_thres))
predicted_class_df[,curr_col_name] <- convert_prediction_function(prediction_df[,"pred_Method2_AIMODEL"],curr_thres)
}
#Add predicted classes to prediction df
comb_df <- cbind(prediction_df,predicted_class_df)
return(comb_df)
}
#Get predicted Class by different threshold
ths <- seq(0.1,0.8,0.1)
test_prediction_df <- add_predicted_class_byThreshold(pred_df,ths)
View(test_prediction_df)
perf_tb_alltest <- get_perf_table_func(test_prediction_df,pts_level_char_df)
get_perf_table_func<- function(analysis_df,pts_level_char_df){
#analysis_df <- pred_df
#Get pre or post numbers
n_pre               <- length(which(analysis_df[,"actual"]==0))
n_post              <- length(which(analysis_df[,"actual"]==1))
#Get SBCE or not patient IDs
analysis_char_df    <- pts_level_char_df[which(pts_level_char_df[,"study_id"] %in% analysis_df[,"study_id"]),]
n_nonrecurrent_pt   <- length(which(analysis_char_df[,"SBCE"]==0))
n_recurrent_pt      <- length(which(analysis_char_df[,"SBCE"]==1))
#Get performance for each threshold
thres_class_cols <- colnames(analysis_df)[which(grepl("Thres",colnames(analysis_df))==T)]
final_perf_df<- as.data.frame(matrix(NA, nrow = length(thres_class_cols),ncol = 14))
colnames(final_perf_df) <- c("Threshold","N_NonRecurrent","N_Recurrent","N_Pre","N_Post",
"AUC","Accuracy","Recall/Sensitivity/TPR",
"Specificity/TNR",
"Precision/PPV","F1",
"NPV","FPR","FNR")
for (i in 1:length(thres_class_cols)){
final_perf_df[i,"Threshold"]          <- thres_class_cols[i]
final_perf_df[i,"N_NonRecurrent"]     <- n_nonrecurrent_pt
final_perf_df[i,"N_Recurrent"]        <- n_recurrent_pt
final_perf_df[i,"N_Pre"]              <- n_pre
final_perf_df[i,"N_Post"]             <- n_post
curr_perf <- compute_binaryclass_perf_func2(analysis_df,thres_class_cols[i])
final_perf_df[i,"AUC"]                    <- curr_perf["AUC"]
final_perf_df[i,"Accuracy"]               <- curr_perf["Accuracy"]
final_perf_df[i,"Recall/Sensitivity/TPR"] <- curr_perf["Recall"]
final_perf_df[i,"Specificity/TNR"]        <- curr_perf["Specificity"]
final_perf_df[i,"Precision/PPV"]          <- curr_perf["Precision"]
final_perf_df[i,"F1"]                     <- curr_perf["F1"]
final_perf_df[i,"NPV"]                    <- curr_perf["Neg Pred Value"]
final_perf_df[i,"FPR"]                    <- curr_perf["FPR"]
final_perf_df[i,"FNR"]                    <- curr_perf["FNR"]
}
return(final_perf_df)
}
perf_tb_alltest <- get_perf_table_func(test_prediction_df,pts_level_char_df)
test_prediction_df$y_PRE_OR_POST_2ndEvent
analysis_df <- test_prediction_df
colnames(analysis_df)
#Get pre or post numbers
n_pre               <- length(which(analysis_df[,"y_PRE_OR_POST_2ndEvent"]==0))
n_post              <- length(which(analysis_df[,"y_PRE_OR_POST_2ndEvent"]==1))
#Get SBCE or not patient IDs
analysis_char_df    <- pts_level_char_df[which(pts_level_char_df[,"study_id"] %in% analysis_df[,"study_id"]),]
n_nonrecurrent_pt   <- length(which(analysis_char_df[,"SBCE"]==0))
n_recurrent_pt      <- length(which(analysis_char_df[,"SBCE"]==1))
#Get performance for each threshold
thres_class_cols <- colnames(analysis_df)[which(grepl("Thres",colnames(analysis_df))==T)]
final_perf_df<- as.data.frame(matrix(NA, nrow = length(thres_class_cols),ncol = 14))
colnames(final_perf_df) <- c("Threshold","N_NonRecurrent","N_Recurrent","N_Pre","N_Post",
"AUC","Accuracy","Recall/Sensitivity/TPR",
"Specificity/TNR",
"Precision/PPV","F1",
"NPV","FPR","FNR")
for (i in 1:length(thres_class_cols)){
final_perf_df[i,"Threshold"]          <- thres_class_cols[i]
final_perf_df[i,"N_NonRecurrent"]     <- n_nonrecurrent_pt
final_perf_df[i,"N_Recurrent"]        <- n_recurrent_pt
final_perf_df[i,"N_Pre"]              <- n_pre
final_perf_df[i,"N_Post"]             <- n_post
curr_perf <- compute_binaryclass_perf_func2(analysis_df,thres_class_cols[i])
final_perf_df[i,"AUC"]                    <- curr_perf["AUC"]
final_perf_df[i,"Accuracy"]               <- curr_perf["Accuracy"]
final_perf_df[i,"Recall/Sensitivity/TPR"] <- curr_perf["Recall"]
final_perf_df[i,"Specificity/TNR"]        <- curr_perf["Specificity"]
final_perf_df[i,"Precision/PPV"]          <- curr_perf["Precision"]
final_perf_df[i,"F1"]                     <- curr_perf["F1"]
final_perf_df[i,"NPV"]                    <- curr_perf["Neg Pred Value"]
final_perf_df[i,"FPR"]                    <- curr_perf["FPR"]
final_perf_df[i,"FNR"]                    <- curr_perf["FNR"]
}
compute_binaryclass_perf_func2 <- function(prediction_df,thresholdClass_col){
#prediction_df <- analysis_df
#thresholdClass_col <- thres_class_cols[i]
predicted_prob  <- prediction_df[,"pred"]
predicted_class <- prediction_df[,thresholdClass_col]
actual_label    <- prediction_df[,"actual"]
#compute ROC-AUC
auc_res <- compute_auc_func(predicted_prob,actual_label)
auc_score <- as.numeric(auc_res[[1]])
#Match label factor levels
matched_res   <- match_label_levels_func(predicted_class,actual_label)
final_pred    <- matched_res[[1]]
final_actual  <- matched_res[[2]]
cm<-confusionMatrix(final_pred, final_actual, positive = "1", dnn = c("Prediction", "TrueLabels"),mode = "everything")
#Manually get TN, FP, TP, FN
TN <- length(which(final_pred==0 & final_actual==0))
FP <- length(which(final_pred==1 & final_actual==0))
TP <- length(which(final_pred==1 & final_actual==1))
FN <- length(which(final_pred==0 & final_actual==1))
#cm_tb <- cm$table
# TN <- cm_tb[1,1]
# FP <- cm_tb[2,1]
# TP <- cm_tb[2,2]
# FN <- cm_tb[1,2]
#class 1
performance_table <- cm$byClass[c("Sensitivity","Specificity",
"Pos Pred Value","Neg Pred Value",
"Precision", "Recall","F1")]
performance_table["Accuracy"] <- cm$overall[1]
performance_table["AUC"]      <- auc_score
performance_table["TNR_Specificity"] <- TN/(TN + FP) #True negative rate = specificity
performance_table["FPR"] <- FP/(TN + FP) #False postive rate
performance_table["TPR_Sensitivity_Recall"] <- TP/(TP + FN) #True postive rate = sensitivity = recall
performance_table["FNR"] <- FN/(TP + FN) #False negative rate
performance_table <- round(performance_table,2)
return(performance_table)
}
for (i in 1:length(thres_class_cols)){
final_perf_df[i,"Threshold"]          <- thres_class_cols[i]
final_perf_df[i,"N_NonRecurrent"]     <- n_nonrecurrent_pt
final_perf_df[i,"N_Recurrent"]        <- n_recurrent_pt
final_perf_df[i,"N_Pre"]              <- n_pre
final_perf_df[i,"N_Post"]             <- n_post
curr_perf <- compute_binaryclass_perf_func2(analysis_df,thres_class_cols[i])
final_perf_df[i,"AUC"]                    <- curr_perf["AUC"]
final_perf_df[i,"Accuracy"]               <- curr_perf["Accuracy"]
final_perf_df[i,"Recall/Sensitivity/TPR"] <- curr_perf["Recall"]
final_perf_df[i,"Specificity/TNR"]        <- curr_perf["Specificity"]
final_perf_df[i,"Precision/PPV"]          <- curr_perf["Precision"]
final_perf_df[i,"F1"]                     <- curr_perf["F1"]
final_perf_df[i,"NPV"]                    <- curr_perf["Neg Pred Value"]
final_perf_df[i,"FPR"]                    <- curr_perf["FPR"]
final_perf_df[i,"FNR"]                    <- curr_perf["FNR"]
}
colnames(test_prediction_df)
which(colnames(test_prediction_df) == "pred_Method2_AIMODE")
which(colnames(test_prediction_df) == "pred_Method2_AIMODEL")
colnames(test_prediction_df)[which(colnames(test_prediction_df) == "pred_Method2_AIMODEL")]
colnames(test_prediction_df)[which(colnames(test_prediction_df) == "pred_Method2_AIMODEL")] <- "pred"
perf_tb_alltest <- get_perf_table_func(test_prediction_df,pts_level_char_df)
colnames(test_prediction_df)
colnames(test_prediction_df)[which(colnames(test_prediction_df) == "y_PRE_OR_POST_2ndEvent")] <- "actual"
perf_tb_alltest <- get_perf_table_func(test_prediction_df,pts_level_char_df)
View(perf_tb_alltest)
pred_df <- read.csv(paste0(proj_dir, "DS3/Prediction_Table/pred_tb_all.csv"),stringsAsFactors = F)
#Get predicted Class by different threshold
ths <- seq(0.1,0.8,0.1)
test_prediction_df <- add_predicted_class_byThreshold(pred_df,ths)
colnames(test_prediction_df)[which(colnames(test_prediction_df) == "pred_Method2_AIMODEL")] <- "pred"
colnames(test_prediction_df)[which(colnames(test_prediction_df) == "y_PRE_OR_POST_2ndEvent")] <- "actual"
perf_tb_alltest <- get_perf_table_func(test_prediction_df,pts_level_char_df)
#Get predicted Class by different threshold
ths <- seq(0.1,0.8,0.1)
test_prediction_df <- add_predicted_class_byThreshold(pred_df,ths)
colnames(test_prediction_df)[which(colnames(test_prediction_df) == "pred_Method2_AIMODEL")] <- "pred"
colnames(test_prediction_df)[which(colnames(test_prediction_df) == "y_PRE_OR_POST_2ndEvent")] <- "actual"
perf_tb_alltest <- get_perf_table_func(test_prediction_df,pts_level_char_df)
View(perf_tb_alltest)
proj_dir <- '/Users/lucasliu/Desktop/DrChen_Projects/ReCAPSE_Project/ReCAPSE_Intermediate_Data/0610_21/16C_Predictions/Test/'
pred_df <- read.csv(paste0(proj_dir, "DS1/Prediction_Table/pred_tb_all.csv"),stringsAsFactors = F)
#Get predicted Class by different threshold
ths <- seq(0.1,0.8,0.1)
test_prediction_df <- add_predicted_class_byThreshold(pred_df,ths)
colnames(test_prediction_df)[which(colnames(test_prediction_df) == "pred_Method2_AIMODEL")] <- "pred"
colnames(test_prediction_df)[which(colnames(test_prediction_df) == "y_PRE_OR_POST_2ndEvent")] <- "actual"
perf_tb_alltest <- get_perf_table_func(test_prediction_df,pts_level_char_df)
perf_tb_alltest
pred_df <- read.csv(paste0(proj_dir, "DS2/Prediction_Table/pred_tb_all.csv"),stringsAsFactors = F)
#Get predicted Class by different threshold
ths <- seq(0.1,0.8,0.1)
test_prediction_df <- add_predicted_class_byThreshold(pred_df,ths)
colnames(test_prediction_df)[which(colnames(test_prediction_df) == "pred_Method2_AIMODEL")] <- "pred"
colnames(test_prediction_df)[which(colnames(test_prediction_df) == "y_PRE_OR_POST_2ndEvent")] <- "actual"
perf_tb_alltest <- get_perf_table_func(test_prediction_df,pts_level_char_df)
perf_tb_alltest
pred_df <- read.csv(paste0(proj_dir, "DS8/Prediction_Table/pred_tb_all.csv"),stringsAsFactors = F)
#Get predicted Class by different threshold
ths <- seq(0.1,0.8,0.1)
test_prediction_df <- add_predicted_class_byThreshold(pred_df,ths)
colnames(test_prediction_df)[which(colnames(test_prediction_df) == "pred_Method2_AIMODEL")] <- "pred"
colnames(test_prediction_df)[which(colnames(test_prediction_df) == "y_PRE_OR_POST_2ndEvent")] <- "actual"
perf_tb_alltest <- get_perf_table_func(test_prediction_df,pts_level_char_df)
perf_tb_alltest
source('~/Desktop/DrChen_Projects/ReCAPSE_Project/ReCAPSE_Code/onHPC/16B_Train_Xgboost_Topfeatures.R')
source('~/Desktop/DrChen_Projects/ReCAPSE_Project/ReCAPSE_Code/onHPC/16B_Train_Xgboost_Topfeatures.R')
source('~/Desktop/DrChen_Projects/ReCAPSE_Project/ReCAPSE_Code/onHPC/16D_Prediction_TestData.R')
source('~/Desktop/DrChen_Projects/ReCAPSE_Project/ReCAPSE_Code/onHPC/check.R')
perf_tb_alltest
pred_df <- read.csv(paste0(proj_dir, "DS5/Prediction_Table/pred_tb_all.csv"),stringsAsFactors = F)
#Get predicted Class by different threshold
ths <- seq(0.1,0.8,0.1)
test_prediction_df <- add_predicted_class_byThreshold(pred_df,ths)
colnames(test_prediction_df)[which(colnames(test_prediction_df) == "pred_Method2_AIMODEL")] <- "pred"
colnames(test_prediction_df)[which(colnames(test_prediction_df) == "y_PRE_OR_POST_2ndEvent")] <- "actual"
perf_tb_alltest <- get_perf_table_func(test_prediction_df,pts_level_char_df)
get_perf_table_func<- function(analysis_df,pts_level_char_df){
#Get pre or post numbers
n_pre               <- length(which(analysis_df[,"actual"]==0))
n_post              <- length(which(analysis_df[,"actual"]==1))
#Get SBCE or not patient IDs
analysis_char_df    <- pts_level_char_df[which(pts_level_char_df[,"study_id"] %in% analysis_df[,"study_id"]),]
n_nonrecurrent_pt   <- length(which(analysis_char_df[,"SBCE"]==0))
n_recurrent_pt      <- length(which(analysis_char_df[,"SBCE"]==1))
#Get performance for each threshold
thres_class_cols <- colnames(analysis_df)[which(grepl("Thres",colnames(analysis_df))==T)]
final_perf_df<- as.data.frame(matrix(NA, nrow = length(thres_class_cols),ncol = 14))
colnames(final_perf_df) <- c("Threshold","N_NonRecurrent","N_Recurrent","N_Pre","N_Post",
"AUC","Accuracy","Recall/Sensitivity/TPR",
"Specificity/TNR",
"Precision/PPV","F1",
"NPV","FPR","FNR")
for (i in 1:length(thres_class_cols)){
final_perf_df[i,"Threshold"]          <- thres_class_cols[i]
final_perf_df[i,"N_NonRecurrent"]     <- n_nonrecurrent_pt
final_perf_df[i,"N_Recurrent"]        <- n_recurrent_pt
final_perf_df[i,"N_Pre"]              <- n_pre
final_perf_df[i,"N_Post"]             <- n_post
curr_perf <- compute_binaryclass_perf_func2(analysis_df,thres_class_cols[i])
final_perf_df[i,"AUC"]                    <- curr_perf["AUC"]
final_perf_df[i,"Accuracy"]               <- curr_perf["Accuracy"]
final_perf_df[i,"Recall/Sensitivity/TPR"] <- curr_perf["Recall"]
final_perf_df[i,"Specificity/TNR"]        <- curr_perf["Specificity"]
final_perf_df[i,"Precision/PPV"]          <- curr_perf["Precision"]
final_perf_df[i,"F1"]                     <- curr_perf["F1"]
final_perf_df[i,"NPV"]                    <- curr_perf["Neg Pred Value"]
final_perf_df[i,"FPR"]                    <- curr_perf["FPR"]
final_perf_df[i,"FNR"]                    <- curr_perf["FNR"]
}
return(final_perf_df)
}
perf_tb_alltest <- get_perf_table_func(test_prediction_df,pts_level_char_df)
proj_dir  <- "/Users/lucasliu/Desktop/DrChen_Projects/ReCAPSE_Project/ReCAPSE_Intermediate_Data/0610_21/"
pred_df <- read.csv(paste0(proj_dir, "16C_Predictions/Test/DS5/Prediction_Table/pred_tb_all.csv"),stringsAsFactors = F)
#Get predicted Class by different threshold
ths <- seq(0.1,0.8,0.1)
test_prediction_df <- add_predicted_class_byThreshold(pred_df,ths)
colnames(test_prediction_df)[which(colnames(test_prediction_df) == "pred_Method2_AIMODEL")] <- "pred"
colnames(test_prediction_df)[which(colnames(test_prediction_df) == "y_PRE_OR_POST_2ndEvent")] <- "actual"
data_dir2        <- paste0(proj_dir, "8_Characteristics2/Patient_Level/")
pts_level_char_df <- read.xlsx(paste0(data_dir2,"/8_PatientLevel_char_WithPossibleMonthsHasNoCodes.xlsx"),sheet = 1)
perf_tb_alltest <- get_perf_table_func(test_prediction_df,pts_level_char_df)
add_predicted_class_byThreshold <- function(prediction_df,thres_list){
predicted_class_df <- as.data.frame(matrix(NA, nrow = nrow(prediction_df),ncol = length(thres_list)))
colnames(predicted_class_df) <- gsub("\\.","", paste0("PredictedClass_Thres_",thres_list))
for (j in 1:length(thres_list)){
curr_thres <- thres_list[j]
curr_col_name <- gsub("\\.","",paste0("PredictedClass_Thres_",curr_thres))
predicted_class_df[,curr_col_name] <- convert_prediction_function(prediction_df[,"pred_Method2_AIMODEL"],curr_thres)
}
#Add predicted classes to prediction df
comb_df <- cbind(prediction_df,predicted_class_df)
return(comb_df)
}
compute_binaryclass_perf_func2 <- function(prediction_df,thresholdClass_col){
#prediction_df <- analysis_df
#thresholdClass_col <- thres_class_cols[i]
predicted_prob  <- prediction_df[,"pred"]
predicted_class <- prediction_df[,thresholdClass_col]
actual_label    <- prediction_df[,"actual"]
#compute ROC-AUC
auc_res <- compute_auc_func(predicted_prob,actual_label)
auc_score <- as.numeric(auc_res[[1]])
#Match label factor levels
matched_res   <- match_label_levels_func(predicted_class,actual_label)
final_pred    <- matched_res[[1]]
final_actual  <- matched_res[[2]]
cm<-confusionMatrix(final_pred, final_actual, positive = "1", dnn = c("Prediction", "TrueLabels"),mode = "everything")
#Manually get TN, FP, TP, FN
TN <- length(which(final_pred==0 & final_actual==0))
FP <- length(which(final_pred==1 & final_actual==0))
TP <- length(which(final_pred==1 & final_actual==1))
FN <- length(which(final_pred==0 & final_actual==1))
#cm_tb <- cm$table
# TN <- cm_tb[1,1]
# FP <- cm_tb[2,1]
# TP <- cm_tb[2,2]
# FN <- cm_tb[1,2]
#class 1
performance_table <- cm$byClass[c("Sensitivity","Specificity",
"Pos Pred Value","Neg Pred Value",
"Precision", "Recall","F1")]
performance_table["Accuracy"] <- cm$overall[1]
performance_table["AUC"]      <- auc_score
performance_table["TNR_Specificity"] <- TN/(TN + FP) #True negative rate = specificity
performance_table["FPR"] <- FP/(TN + FP) #False postive rate
performance_table["TPR_Sensitivity_Recall"] <- TP/(TP + FN) #True postive rate = sensitivity = recall
performance_table["FNR"] <- FN/(TP + FN) #False negative rate
performance_table <- round(performance_table,2)
return(performance_table)
}
perf_tb_alltest <- get_perf_table_func(test_prediction_df,pts_level_char_df)
perf_tb_alltest
perf_tb_alltest[5,]
source('~/Desktop/DrChen_Projects/ReCAPSE_Project/ReCAPSE_Code/onHPC/16B_Train_Xgboost_Topfeatures.R')
source('~/Desktop/DrChen_Projects/ReCAPSE_Project/ReCAPSE_Code/onHPC/16D_Prediction_TestData.R')
source('~/Desktop/DrChen_Projects/ReCAPSE_Project/ReCAPSE_Code/onHPC/check.R')
perf_tb_alltest[5,]
source('~/Desktop/DrChen_Projects/ReCAPSE_Project/ReCAPSE_Code/onHPC/16B_Train_Xgboost_Topfeatures.R')
source('~/Desktop/DrChen_Projects/ReCAPSE_Project/ReCAPSE_Code/onHPC/16D_Prediction_TestData.R')
source('~/Desktop/DrChen_Projects/ReCAPSE_Project/ReCAPSE_Code/onHPC/check.R')
perf_tb_alltest[5,]
source('~/Desktop/DrChen_Projects/ReCAPSE_Project/ReCAPSE_Code/onHPC/check.R')
perf_tb_alltest[5,]
