#External Validation
external_res<-main_external_func(train_data,external_validation_data,features_to_select,label_col_name, top_feature_flag,critical_features,upsample_flag,num_sampling,xgb_params,num_rounds)
external_AVG_performance<-round(external_res[[1]],2)
external_predicted_list <- external_res[[2]] #this variable contains list of tables of predicted values in each sampling
#Print external performance
print(external_AVG_performance)
#Print external predicted values
example_prediction <- external_predicted_list[[1]]
print(example_prediction)
#
##User input Pamameters
label_col_name <- "outcome"
features_to_select <- colnames(data_input)[3:6]
n_class <- 2
important_weight_threshold <- 0.2
top_feature_flag <- 1
upsample_flag <- 1
num_rounds <- 10
num_sampling <- 5
#For multi-class classification #for multi-class, num_class needs to be specified:
#xgb_params <- list(booster = "gbtree","objective" = "multi:softmax",num_class = n_class)
#For binary classification:
xgb_params <- list(booster = "gbtree","objective" = "reg:logistic")
##LOOCV
LOOCV_res<-main_func(train_data,features_to_select,label_col_name, top_feature_flag,important_weight_threshold,upsample_flag,num_sampling,xgb_params,num_rounds)
features_to_select <- colnames(data_input)[3:6]
# ############               Final run my code
# ########################################################################################################
##User input Pamameters
label_col_name <- "outcome"
features_to_select <- colnames(train_data)[3:6]
n_class <- 2
important_weight_threshold <- 0.2
top_feature_flag <- 1
upsample_flag <- 1
num_rounds <- 10
num_sampling <- 5
#For multi-class classification #for multi-class, num_class needs to be specified:
#xgb_params <- list(booster = "gbtree","objective" = "multi:softmax",num_class = n_class)
#For binary classification:
xgb_params <- list(booster = "gbtree","objective" = "reg:logistic")
##LOOCV
LOOCV_res<-main_func(train_data,features_to_select,label_col_name, top_feature_flag,important_weight_threshold,upsample_flag,num_sampling,xgb_params,num_rounds)
importantce_Matrix<-LOOCV_res[[1]]
LOOCV_AVG_performance<-round(LOOCV_res[[2]],2)
critical_features <- LOOCV_res[[3]]
#Print LOOCV performance
print(LOOCV_AVG_performance)
#Plot top feature importance
final_top_p <- Plot_FeatureImportance_func(importantce_Matrix,1,critical_features)
print(final_top_p)
#Plot all feature importance
final_p <- Plot_FeatureImportance_func(importantce_Matrix,0,critical_features)
print(final_p)
#External Validation
external_res<-main_external_func(train_data,external_validation_data,features_to_select,label_col_name, top_feature_flag,critical_features,upsample_flag,num_sampling,xgb_params,num_rounds)
external_AVG_performance<-round(external_res[[1]],2)
external_predicted_list <- external_res[[2]] #this variable contains list of tables of predicted values in each sampling
#Print external performance
print(external_AVG_performance)
pred
compute_predclass <- function(pred_prob){
pred_class <- NA
for (i in 1:length(pred_prob)){
if(pred_prob[i] >= 0.5){
pred_class[i] <- 1
}else{
pred_class[i] <- 0
}
}
return(pred_class)
}
pred_class <- compute_predclass(pred)
pred_class
compute_binaryclass_perf_func(pred,actual)
pred <- predict(mod_optimal, dtest)
actual <-test_label
compute_binaryclass_perf_func(pred,actual)
source('~/Desktop/DrChen_Projects/ReCAPSE_Project/ReCAPSE_Code/Run_Xgboost.R', echo=TRUE)
table(comb_data_input$outcome)
#remove all Na columns
comb_data_input <- comb_data_input[,colSums(is.na(comb_data_input))<nrow(comb_data_input)]
#split train and test
set.seed(123)
library(caTools)
sample <- sample.split(comb_data_input,SplitRatio = 0.8) # 0.8 for training
train_data <- subset(comb_data_input,sample ==TRUE)
external_validation_data <- subset(comb_data_input, sample==FALSE)
table(train_data$outcome)
table(external_validation_data$outcome)
# ########################################################################################################
#           Teresa's code
# ########################################################################################################
library(rBayesianOptimization)
library(xgboost)
library(Matrix)
train_label <- train_data[,"outcome"]
train_label <- as.numeric(train_label)
train_data_part<-train_data[,!(names(train_data) %in% c("ID","Month_Start","outcome"))]
dtrain <- xgb.DMatrix(data = as.matrix(train_data_part), label = train_label)
test_label <- external_validation_data[,"outcome"]
test_label <- as.numeric(test_label)
test_data_part<-external_validation_data[,!(names(external_validation_data) %in% c("ID","Month_Start","outcome"))]
dtest <- xgb.DMatrix(data = as.matrix(test_data_part), label = test_label)
xgb_cv_bayes <- function(eta, max_depth, min_child_weight, subsample, colsample_by_tree){
print(paste("eta:", eta))
print(paste("max_depth:", max_depth))
print(paste("min_child_weight:", min_child_weight))
print(paste("subsample:", subsample))
print(paste("colsample_by_tree:", colsample_by_tree))
cv <- xgb.cv(params=list(booster="gbtree", eta=eta, max_depth=max_depth,
min_child_weight=min_child_weight,
subsample=subsample,
olsample_by_tree=colsample_by_tree,
lambda=1, alpha=0,
#nthread=ncores, n_jobs=ncores,
objective="binary:logistic", eval_metric="auc"),
data=dtrain, nround=5,nfold = 10,
prediction=TRUE, showsd=TRUE, early_stopping_rounds=100,
stratified=FALSE, maximize=TRUE)
print(paste("cv:", cv))
list(Score=cv$evaluation_log[, max(test_auc_mean)], Pred=cv$pred)
}
optimal_results <- BayesianOptimization(xgb_cv_bayes,
bounds=list(eta=c(0.001, 0.3),
max_depth=c(3L, 10L),
min_child_weight=c(0L, 20L),
subsample=c(0.3, 0.9), colsample_by_tree=c(0.2, 0.8)),
init_points=10,
n_iter=10)
current_best <- list(etc = 0.1689410, max_depth = 8,min_child_weight = 2, subsample = 0.3780268,
colsample_by_tree = 0.7334129)
watchlist <- list(train = dtrain, eval = dtest)
mod_optimal <- xgb.train(objective="binary:logistic",
params=current_best, data=dtrain, nrounds=10, early_stopping_rounds=100, maximize=TRUE,
watchlist=watchlist, verbose=TRUE, print_every_n=10, eval_metric="error", eval_metric="error@0.2", eval_metric="auc")
pred <- predict(mod_optimal, dtest)
actual <-test_label
compute_binaryclass_perf_func(pred,actual)
optimal_results$Best_Par
current_best <- list(etc = 0.001, max_depth = 10,min_child_weight = 3, subsample = 0.7987099,
colsample_by_tree = 0.7366271)
watchlist <- list(train = dtrain, eval = dtest)
mod_optimal <- xgb.train(objective="binary:logistic",
params=current_best, data=dtrain, nrounds=10, early_stopping_rounds=100, maximize=TRUE,
watchlist=watchlist, verbose=TRUE, print_every_n=10, eval_metric="error", eval_metric="error@0.2", eval_metric="auc")
pred <- predict(mod_optimal, dtest)
actual <-test_label
compute_binaryclass_perf_func(pred,actual)
nrow(recurrent_pts_data)
nrow(data_input2)
nrow(nonrecurrent_pts_data)
optimal_results$Best_Par['eta']
current_best <- list(etc = optimal_results$Best_Par['eta'],
max_depth = optimal_results$Best_Par['max_depth'],
min_child_weight = optimal_results$Best_Par['min_child_weight'],
subsample = optimal_results$Best_Par['subsample'],
colsample_by_tree = optimal_results$Best_Par['colsample_by_tree'])
current_best
current_best <- list(etc = optimal_results$Best_Par['eta'][1],
max_depth = optimal_results$Best_Par['max_depth'],
min_child_weight = optimal_results$Best_Par['min_child_weight'],
subsample = optimal_results$Best_Par['subsample'],
colsample_by_tree = optimal_results$Best_Par['colsample_by_tree'])
current_best
optimal_results$Best_Par[1,'eta']
optimal_results$Best_Par['eta']
as.numeric(optimal_results$Best_Par[1,'eta'])
as.numeric(optimal_results$Best_Par['eta'])
current_best <- list(etc = as.numeric(optimal_results$Best_Par[1,'eta']),
max_depth = as.numeric(optimal_results$Best_Par['max_depth']),
min_child_weight = as.numeric(optimal_results$Best_Par['min_child_weight']),
subsample = as.numeric(optimal_results$Best_Par['subsample']),
colsample_by_tree = as.numeric(optimal_results$Best_Par['colsample_by_tree']))
current_best <- list(etc = as.numeric(optimal_results$Best_Par['eta']),
max_depth = as.numeric(optimal_results$Best_Par['max_depth']),
min_child_weight = as.numeric(optimal_results$Best_Par['min_child_weight']),
subsample = as.numeric(optimal_results$Best_Par['subsample']),
colsample_by_tree = as.numeric(optimal_results$Best_Par['colsample_by_tree']))
current_best
watchlist <- list(train = dtrain, eval = dtest)
mod_optimal <- xgb.train(objective="binary:logistic",
params=current_best, data=dtrain, nrounds=10, early_stopping_rounds=100, maximize=TRUE,
watchlist=watchlist, verbose=TRUE, print_every_n=10, eval_metric="error", eval_metric="error@0.2", eval_metric="auc")
pred <- predict(mod_optimal, dtest)
actual <-test_label
compute_binaryclass_perf_func(pred,actual)
nrow(nonrecurrent_pts_data)
nrow(recurrent_pts_data)
##########START HERE##########
########################################################################################################
###############              Data preprocessing
###############     Note: Make sure label range: [0,num_class-1]
########################################################################################################
data_input1 <- recurrent_pts_data[sample(nrow(recurrent_pts_data), nrow(recurrent_pts_data), replace = FALSE),]
data_input2 <- nonrecurrent_pts_data[sample(nrow(nonrecurrent_pts_data), nrow(recurrent_pts_data), replace = FALSE),]
comb_data_input <- rbind(data_input1,data_input2)
table(comb_data_input$outcome)
#remove all Na columns
comb_data_input <- comb_data_input[,colSums(is.na(comb_data_input))<nrow(comb_data_input)]
#split train and test
set.seed(123)
library(caTools)
sample <- sample.split(comb_data_input,SplitRatio = 0.8) # 0.8 for training
train_data <- subset(comb_data_input,sample ==TRUE)
external_validation_data <- subset(comb_data_input, sample==FALSE)
table(train_data$outcome)
table(external_validation_data$outcome)
# ########################################################################################################
#           Teresa's code
# ########################################################################################################
library(rBayesianOptimization)
library(xgboost)
library(Matrix)
train_label <- train_data[,"outcome"]
train_label <- as.numeric(train_label)
train_data_part<-train_data[,!(names(train_data) %in% c("ID","Month_Start","outcome"))]
dtrain <- xgb.DMatrix(data = as.matrix(train_data_part), label = train_label)
test_label <- external_validation_data[,"outcome"]
test_label <- as.numeric(test_label)
test_data_part<-external_validation_data[,!(names(external_validation_data) %in% c("ID","Month_Start","outcome"))]
dtest <- xgb.DMatrix(data = as.matrix(test_data_part), label = test_label)
xgb_cv_bayes <- function(eta, max_depth, min_child_weight, subsample, colsample_by_tree){
print(paste("eta:", eta))
print(paste("max_depth:", max_depth))
print(paste("min_child_weight:", min_child_weight))
print(paste("subsample:", subsample))
print(paste("colsample_by_tree:", colsample_by_tree))
cv <- xgb.cv(params=list(booster="gbtree", eta=eta, max_depth=max_depth,
min_child_weight=min_child_weight,
subsample=subsample,
olsample_by_tree=colsample_by_tree,
lambda=1, alpha=0,
#nthread=ncores, n_jobs=ncores,
objective="binary:logistic", eval_metric="auc"),
data=dtrain, nround=5,nfold = 10,
prediction=TRUE, showsd=TRUE, early_stopping_rounds=100,
stratified=FALSE, maximize=TRUE)
print(paste("cv:", cv))
list(Score=cv$evaluation_log[, max(test_auc_mean)], Pred=cv$pred)
}
optimal_results <- BayesianOptimization(xgb_cv_bayes,
bounds=list(eta=c(0.001, 0.3),
max_depth=c(3L, 10L),
min_child_weight=c(0L, 20L),
subsample=c(0.3, 0.9), colsample_by_tree=c(0.2, 0.8)),
init_points=10,
n_iter=10)
current_best <- list(etc = as.numeric(optimal_results$Best_Par['eta']),
max_depth = as.numeric(optimal_results$Best_Par['max_depth']),
min_child_weight = as.numeric(optimal_results$Best_Par['min_child_weight']),
subsample = as.numeric(optimal_results$Best_Par['subsample']),
colsample_by_tree = as.numeric(optimal_results$Best_Par['colsample_by_tree']))
watchlist <- list(train = dtrain, eval = dtest)
mod_optimal <- xgb.train(objective="binary:logistic",
params=current_best, data=dtrain, nrounds=10, early_stopping_rounds=100, maximize=TRUE,
watchlist=watchlist, verbose=TRUE, print_every_n=10, eval_metric="error", eval_metric="error@0.2", eval_metric="auc")
pred <- predict(mod_optimal, dtest)
actual <-test_label
compute_binaryclass_perf_func(pred,actual)
proj_dir <- "/Users/lucasliu/Desktop/DrChen_Projects/ReCAPSE_Project/"
data_dir <- paste0(proj_dir,"/ReCAPSE_Intermediate_Data/0318_21/For_Both_Data/")
month_data <- read.csv(paste0(data_dir, "diag_monthly_df.csv"))
month_data <- month_data[,-5] #remove Na code columns
length(unique(month_data$ID)) #27607
nrow(month_data) #2095430
table(month_data$outcome) #0:1985309, 1:110121
####Load patinet char file
data_dir2 <- paste0(proj_dir,"/ReCAPSE_Data/Testing data for UH3 - Dec 16 2020/")
kcr_data <- read.csv(paste0(data_dir2, "uh3_kcrdata.csv"))
View(kcr_data)
i < -1
i <- 1
month_data$ID[i]
curr_id <- month_data$ID[i]
curr_id
curr_id <- month_data[i,"ID"]
which(kcr_data[,"study_id"] == curr_id)
curr_kcr <- which(kcr_data[,"study_id"] == curr_id)
curr_kcr <- kcr_data[which(kcr_data[,"study_id"] == curr_id),]
View(curr_kcr)
###Load event data so that we can select the correpdoning row in kcr
event_data <- read.csv(paste0(data_dir, "updated_All_event_df.csv"))
View(event_data)
i <- 1
curr_id <- month_data[i,"ID"]
curr_event <- event_data[which(event_data[,"study_id"] == curr_id),]
curr_event <- event_data[which(event_data[,"ID"] == curr_id),]
curr_event
curr_1stevent_date <- curr_event[,"Date_1st_Event"]
curr_1stevent_site <- strplist(curr_event[,"Site_1st_Event"],split("_"))
curr_1stevent_site <- strsplit(curr_event[,"Site_1st_Event"],split("_"))
curr_1stevent_site <- strsplit(curr_event[,"Site_1st_Event"],split("/_"))
curr_1stevent_site <- strsplit(curr_event[,"Site_1st_Event"],split("_"))
curr_1stevent_site <- strsplit(curr_event[,"Site_1st_Event"],split = "_")
curr_1stevent_site <- strsplit(curr_event[,"Site_1st_Event"],split = "/_")
curr_event[,"Site_1st_Event"]
kcr_data <- read.csv(paste0(data_dir2, "uh3_kcrdata.csv"),stringsAsFactors = F)
###Load event data so that we can select the correpdoning row in kcr
event_data <- read.csv(paste0(data_dir, "updated_All_event_df.csv"),stringsAsFactors = F)
curr_1stevent_site <- strsplit(curr_event[,"Site_1st_Event"],split= "_")
i <- 1
curr_id <- month_data[i,"ID"]
curr_event <- event_data[which(event_data[,"ID"] == curr_id),]
curr_1stevent_date <- curr_event[,"Date_1st_Event"]
curr_1stevent_site <- strsplit(curr_event[,"Site_1st_Event"],split= "_")
curr_1stevent_site
curr_1stevent_site <- unlist(strsplit(curr_event[,"Site_1st_Event"],split= "_"))[2]
curr_1stevent_site
colnames(kcr_data)
curr_kcr <- kcr_data[which(kcr_data[,"study_id"] == curr_id &
kcr_data[,"PrimarySite"] ==  curr_1stevent_site &
kcr_data[,"Date_dx"] ==  curr_1stevent_date),]
curr_kcr
curr_1stevent_date
curr_dob <- curr_kcr$date_Birth
curr_dob
View(month_data)
colnames(month_data)
curr_month <- month_data[i,"Month_Start"]
curr_month
month_data <- read.csv(paste0(data_dir, "diag_monthly_df.csv"),stringsAsFactors = F)
library(lubridate)
month_data <- month_data[,-5] #remove Na code columns
length(unique(month_data$ID)) #27607
nrow(month_data) #2095430
table(month_data$outcome) #0:1985309, 1:110121
####Load patinet char file
data_dir2 <- paste0(proj_dir,"/ReCAPSE_Data/Testing data for UH3 - Dec 16 2020/")
kcr_data <- read.csv(paste0(data_dir2, "uh3_kcrdata.csv"),stringsAsFactors = F)
###Load event data so that we can select the correpdoning row in kcr
event_data <- read.csv(paste0(data_dir, "updated_All_event_df.csv"),stringsAsFactors = F)
i <- 1
curr_id <- month_data[i,"ID"]
curr_month <- month_data[i,"Month_Start"]
curr_event <- event_data[which(event_data[,"ID"] == curr_id),]
curr_1stevent_date <- curr_event[,"Date_1st_Event"]
curr_1stevent_site <- unlist(strsplit(curr_event[,"Site_1st_Event"],split= "_"))[2]
curr_kcr <- kcr_data[which(kcr_data[,"study_id"] == curr_id &
kcr_data[,"PrimarySite"] ==  curr_1stevent_site &
kcr_data[,"Date_dx"] ==  curr_1stevent_date),]
curr_dob <- curr_kcr$date_Birth
curr_dob
curr_dob <- curr_kcr[,"date_Birth"]
curr_dob
curr_month
mdy(curr_kcr[,"date_Birth"])
curr_age <- difftime(mdy(curr_dob), ymd(curr_month), units = "years")
curr_age <- difftime(mdy(curr_dob), ymd(curr_month), units = "year")
curr_age <- difftime(mdy(curr_dob), ymd(curr_month), units = "days")
curr_age
curr_age <- difftime(ymd(curr_month),mdy(curr_dob), units = "days")
curr_age
curr_age <- (difftime(ymd(curr_month),mdy(curr_dob), units = "days"))/365
curr_age
curr_age <- as.numeric(difftime(ymd(curr_month),mdy(curr_dob), units = "days"))/365
curr_age
curr_months_since_dx <- as.numeric(difftime(ymd(curr_month),mdy(curr_1stevent_date), units = "days"))
curr_months_since_dx
curr_month
curr_1stevent_date
7*30
curr_race <- curr_kcr[,"race"]
colnames(curr_kcr)
curr_race <- curr_kcr[,"race1"]
colnames(curr_kcr)
curr_race <- curr_kcr[,"Race1"]
curr_race
curr_site <- curr_kcr[,"site"]
colnames(curr_kcr)
curr_site <- curr_kcr[,"PrimarySite"]
curr_site
curr_stage <- curr_kcr[,"stage"]
curr_grade <- curr_kcr[,"Grade"]
curr_grade
curr_laterality <- curr_kcr[,"Laterality"]
curr_laterality
curr_er_stat <- curr_kcr[,"er_stat"]
curr_er_stat <- curr_kcr[,"er_stat"]
curr_pr_stat <- curr_kcr[,"pr_stat"]
curr_her2_stat <- curr_kcr[,"her2_stat"]
curr_surg_prim_site <- curr_kcr[,"surg_prim_site"]
curr_surg_prim_site <- curr_kcr[,"RXSummSurgPrimSite"]
curr_surg_prim_site
curr_DAJCC_T <- curr_kcr[,"DAJCC_T"]
curr_reg_nodes_exam <- curr_kcr[,"RegNodesExamined"]
curr_reg_nodes_pos <- curr_kcr[,"RegNodesPositive"]
source("XGBUtilities.R")
proj_dir <- "/Users/lucasliu/Desktop/DrChen_Projects/ReCAPSE_Project/"
data_dir <- paste0(proj_dir,"/ReCAPSE_Intermediate_Data/0318_21/For_Both_Data/")
month_data <- read.csv(paste0(data_dir, "diag_monthly_df.csv"))
month_data <- month_data[,-5] #remove Na code columns
length(unique(month_data$ID)) #27607
nrow(month_data) #2095430
table(month_data$outcome) #0:1985309, 1:110121
nonrecurrent_pts_data <- month_data[which(month_data$outcome == 0),]
recurrent_pts_data <- month_data[which(month_data$outcome == 1),]
#Select the groups that match or exceed the threshold of the fraction of patients with at least one code in that group;
#the nonrecurrent patient thresholds are 0.15, 0.15, and 0.05,
#and the recurrent patient thresholds are 0.10, 0.10, and 0.01 for the diagnostic, procedure, and drug groups
perc_pts <- NA
analysis_data <- recurrent_pts_data
for (j in 4:length(analysis_data)){
curr_col <- analysis_data[,j]
n_pts <- length(which(curr_col >= 1)) #n of pts have at least one code in this group
perc_pts[j] <- n_pts/nrow(analysis_data)
}
ccol_indexes1 <- which(perc_pts > 0.05)
perc_pts <- NA
analysis_data <- nonrecurrent_pts_data
for (j in 4:length(analysis_data)){
curr_col <- analysis_data[,j]
n_pts <- length(which(curr_col >= 1)) #n of pts have at least one code in this group
perc_pts[j] <- n_pts/nrow(analysis_data)
}
ccol_indexes2 <- which(perc_pts > 0.05)
comb_indexes <- unique(ccol_indexes1,ccol_indexes2)
comb_filtered_data <- month_data[,c(1,2,3,comb_indexes)]
##########START HERE##########
########################################################################################################
###############              Data preprocessing
###############     Note: Make sure label range: [0,num_class-1]
########################################################################################################
data_input1 <- recurrent_pts_data[sample(nrow(recurrent_pts_data), nrow(recurrent_pts_data), replace = FALSE),]
data_input2 <- nonrecurrent_pts_data[sample(nrow(nonrecurrent_pts_data), nrow(recurrent_pts_data), replace = FALSE),]
comb_data_input <- rbind(data_input1,data_input2)
table(comb_data_input$outcome)
#remove all Na columns
comb_data_input <- comb_data_input[,colSums(is.na(comb_data_input))<nrow(comb_data_input)]
set.seed(123)
library(caTools)
sample <- sample.split(comb_data_input,SplitRatio = 0.8) # 0.8 for training
train_data <- subset(comb_data_input,sample ==TRUE)
external_validation_data <- subset(comb_data_input, sample==FALSE)
table(train_data$outcome)
table(external_validation_data$outcome)
nrow(month_data) #2095430
table(month_data$outcome) #0:1985309, 1:110121
table(month_data$outcome) #0:1985309, 1:110121
View(train_data)
data_input1 <- recurrent_pts_data[sample(nrow(recurrent_pts_data), nrow(recurrent_pts_data), replace = FALSE),]
data_input2 <- nonrecurrent_pts_data[sample(nrow(nonrecurrent_pts_data), nrow(recurrent_pts_data), replace = FALSE),]
comb_data_input <- rbind(data_input1,data_input2)
table(comb_data_input$outcome)
#remove all Na columns
comb_data_input <- comb_data_input[,colSums(is.na(comb_data_input))<nrow(comb_data_input)]
View(comb_data_input)
source("XGBUtilities.R")
proj_dir <- "/Users/lucasliu/Desktop/DrChen_Projects/ReCAPSE_Project/"
data_dir <- paste0(proj_dir,"/ReCAPSE_Intermediate_Data/0318_21/For_Both_Data/")
month_data <- read.csv(paste0(data_dir, "diag_monthly_df.csv"))
month_data <- month_data[,-5] #remove Na code columns
length(unique(month_data$ID)) #27607
nrow(month_data) #2095430
table(month_data$outcome) #0:1985309, 1:110121
nonrecurrent_pts_data <- month_data[which(month_data$outcome == 0),]
recurrent_pts_data <- month_data[which(month_data$outcome == 1),]
#Select the groups that match or exceed the threshold of the fraction of patients with at least one code in that group;
#the nonrecurrent patient thresholds are 0.15, 0.15, and 0.05,
#and the recurrent patient thresholds are 0.10, 0.10, and 0.01 for the diagnostic, procedure, and drug groups
perc_pts <- NA
analysis_data <- recurrent_pts_data
for (j in 4:length(analysis_data)){
curr_col <- analysis_data[,j]
n_pts <- length(which(curr_col >= 1)) #n of pts have at least one code in this group
perc_pts[j] <- n_pts/nrow(analysis_data)
}
ccol_indexes1 <- which(perc_pts > 0.05)
perc_pts <- NA
analysis_data <- nonrecurrent_pts_data
for (j in 4:length(analysis_data)){
curr_col <- analysis_data[,j]
n_pts <- length(which(curr_col >= 1)) #n of pts have at least one code in this group
perc_pts[j] <- n_pts/nrow(analysis_data)
}
ccol_indexes2 <- which(perc_pts > 0.05)
comb_indexes <- unique(ccol_indexes1,ccol_indexes2)
comb_filtered_data <- month_data[,c(1,2,3,comb_indexes)]
##########START HERE##########
########################################################################################################
###############              Data preprocessing
###############     Note: Make sure label range: [0,num_class-1]
########################################################################################################
data_input1 <- recurrent_pts_data[sample(nrow(recurrent_pts_data), nrow(recurrent_pts_data), replace = FALSE),]
data_input2 <- nonrecurrent_pts_data[sample(nrow(nonrecurrent_pts_data), nrow(recurrent_pts_data), replace = FALSE),]
comb_data_input <- rbind(data_input1,data_input2)
table(comb_data_input$outcome)
#remove all Na columns
comb_data_input <- comb_data_input[,colSums(is.na(comb_data_input))<nrow(comb_data_input)]
avg(perc_pts)
mean(perc_pts)
mean(perc_pts,na.rm = T)
mean(perc_pts,na.rm = T)
perc_pts <- NA
analysis_data <- nonrecurrent_pts_data
for (j in 4:length(analysis_data)){
curr_col <- analysis_data[,j]
n_pts <- length(which(curr_col >= 1)) #n of pts have at least one code in this group
perc_pts[j] <- n_pts/nrow(analysis_data)
}
mean(perc_pts,na.rm = T)
perc_pts <- NA
analysis_data <- nonrecurrent_pts_data
for (j in 4:length(analysis_data)){
curr_col <- analysis_data[,j]
n_pts <- length(which(curr_col >= 1)) #n of pts have at least one code in this group
perc_pts[j] <- n_pts/nrow(analysis_data)
}
mean(perc_pts,na.rm = T)
mean(perc_pts,na.rm = T)
2095430*0.007
2095430*0.001
